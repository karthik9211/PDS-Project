{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IUXe7zYSHAhK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Text Preprocessing Libraries\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#Model Building libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "K-l_ab8OUKPH",
    "outputId": "e6b6447e-7c39-4dfe-dfc9-e17af60e1739"
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 225",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Load the dataset from local\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 225"
     ]
    }
   ],
   "source": [
    "#Load the dataset from local\n",
    "df=pd.read_csv('news.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lx9M978iTHBI",
    "outputId": "2bd561d5-9010-47aa-a609-615d3af97484"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "hU4vHg1X7kOj",
    "outputId": "5bfd7b0e-b440-4da1-a40a-b9122da8a882"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Plot the distribution of the labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Pass the entire DataFrame and specify the column for x\n",
    "sns.countplot(x='label', data=df)  # Changed line\n",
    "plt.title('Distribution of Real vs Fake News')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGNVRkqjTLH7"
   },
   "source": [
    "We can see that there are no missing values in any of the feature and the output variable is well balanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0iyUgiWsjMDz",
    "outputId": "3294066c-f815-4cbc-f5b1-56e6df7f46b2"
   },
   "outputs": [],
   "source": [
    "# Replace 'FAKE' with 0 and 'REAL' with 1 in the 'LABEL' column\n",
    "df['LABEL'] = df['label'].replace({'FAKE': 0, 'REAL': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "U2KTlStMjcCn",
    "outputId": "7e1fb5ad-ae0d-4cd0-8a46-5987f41ef501"
   },
   "outputs": [],
   "source": [
    "df=df.drop('label',axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "89H1Tc6I2NdT",
    "outputId": "8c632a39-3888-4a52-c96d-665978feff0b"
   },
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADg5n-564Ko2"
   },
   "source": [
    "We will combine the title and text here:\n",
    "\n",
    "1. The title often summarizes the main idea or the most important aspect of the article, while the text provides detailed information. Combining both can give a more complete picture of the content.\n",
    "\n",
    "2. **Augmenting Short Articles:** For very short articles, the title can provide additional context and information that might not be present in the body text alone.\n",
    "\n",
    "3. **Use in the project:** Fake news articles often have sensational titles that may not match the content. Combining the title and text helps the model to detect such inconsistencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "U9jGxB_rH-hL",
    "outputId": "62ebc2df-ddae-4a35-841d-43210637c503"
   },
   "outputs": [],
   "source": [
    "# combine title and text together\n",
    "df['original'] = df['title'] + ' ' + df['text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "NaNvVVzmIS-O",
    "outputId": "1e8e5780-778b-4566-e648-224804cad825"
   },
   "outputs": [],
   "source": [
    "df['original'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsPSS2oPpaxx"
   },
   "source": [
    "### **Stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLGNrOOb3Ks2"
   },
   "source": [
    "1. **Stop words are a set of commonly used words in a language. Examples of stop words in English are “a,” “the,” “is,” “are,” etc.**\n",
    "\n",
    "2. We have to remove the words which carry little to no useful information at all from our dataset.\n",
    "\n",
    "3. This can be done by maintaining a list of stop words (which can be manually or automatically curated) and preventing all words from your stop word list from being analyzed.\n",
    "\n",
    "4. Stopwords helps us reduce noise, increase effieciency and enhance model performance by reducing dimensionality.\n",
    "\n",
    "In tasks such as document similarity or clustering, stopwords can introduce noise that affects the accuracy of similarity measures. Removing stopwords leads to more accurate and meaningful similarity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NZ2bglCTIcwh",
    "outputId": "1e35dc08-6cf5-46e3-b544-49bff597d6b6"
   },
   "outputs": [],
   "source": [
    "# download stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1BhRkroIitM",
    "outputId": "09bb1774-2259-4bb1-d4e3-3053b2a72ead"
   },
   "outputs": [],
   "source": [
    "# Obtain additional stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# Extend the stopwords list with domain-specific stopwords\n",
    "extra_stopwords = [\n",
    "    'breaking', 'report', 'latest', 'update', 'exclusive', 'headline',\n",
    "    'according', 'sources', 'allegedly', 'reported', 'confirm', 'statement',\n",
    "    'yesterday', 'today', 'tomorrow', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',\n",
    "    'new york', 'washington',\n",
    "    'said', 'added', 'told', 'commented', 'mentioned',\n",
    "    'what', 'how', 'when', 'where', 'who'\n",
    "]\n",
    "\n",
    "stop_words.extend(extra_stopwords)\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1YK1pF_p6GV"
   },
   "source": [
    "### **Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXQrH3NJ4zbA"
   },
   "source": [
    "The simple_preprocess function from Gensim:\n",
    "\n",
    "1. Converts the text to lowercase.\n",
    "2. Tokenizes the text into individual words.\n",
    "3. Removes punctuation.\n",
    "4. Optionally removes words that are too short or too long (though the latter is not explicitly configured here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SGCNnn7IsNV"
   },
   "outputs": [],
   "source": [
    "# Remove stopwords and remove words with 2 or less characters\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):     #tokenization\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n",
    "            result.append(token)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ixu0Hq1JnFY"
   },
   "outputs": [],
   "source": [
    "# Apply the function to the dataframe\n",
    "df['clean'] = df['original'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "VjbCkx8cKSoz",
    "outputId": "0c0c2088-e7a3-432e-c4d9-3af80517e3f4"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwS6oHMa6ohT"
   },
   "source": [
    "**List of Words:**\n",
    "\n",
    "1. By generating a list of all words, we can perform frequency analysis to identify commonly occurring words that might not be informative for our classification task. These frequent but non-discriminative words can be added to our stopwords list to improve the quality of your text data.\n",
    "\n",
    "2. We can do the frequency of word calculations and everything with either CountVectorizer or TFIDF Transformer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaJ5w7gdKiUR"
   },
   "outputs": [],
   "source": [
    "# Obtain the total words present in the dataset\n",
    "list_of_words = []\n",
    "for i in df.clean:\n",
    "    for j in i:\n",
    "        list_of_words.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hf2tUqtJKqfu",
    "outputId": "a7f29161-ec80-4131-fd93-cdafd06344e9"
   },
   "outputs": [],
   "source": [
    "list_of_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZOWXCQz7J7B",
    "outputId": "80537935-17ee-4e28-f4f8-a089f760c1b0"
   },
   "outputs": [],
   "source": [
    "len(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0WUYgmCUdSV",
    "outputId": "40965324-475a-40af-f1e3-0eb7da3b76e0"
   },
   "outputs": [],
   "source": [
    "# Obtain the total number of unique words\n",
    "total_words = len(list(set(list_of_words)))\n",
    "total_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCOQ9Zia-mws"
   },
   "source": [
    "We should now convert lists of cleaned words into single strings in a new column 'clean_joined' in our DataFrame.\n",
    "\n",
    "1. This transformation is often necessary for further text processing tasks\n",
    "like vectorization or modeling, where each document (in this case, a news article) needs to be represented as a single string of text. This process ensures that your data is in a format suitable for subsequent NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "hX3WI0HnUn24",
    "outputId": "458c25bb-23f4-46ef-e3a9-1111fecd8d01"
   },
   "outputs": [],
   "source": [
    "#Join the words into a string\n",
    "# Apply the following lambda function to each element in the 'clean' column of the dataframe 'df':\n",
    "# - For each element 'x' in the 'clean' column, join the list of words in 'x' into a single string with spaces in between.\n",
    "# - Assign the resulting string to a new column called 'clean_joined' in the dataframe 'df'.\n",
    "df['clean_joined'] = df['clean'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Display the first few rows of the dataframe to verify the new column has been added and populated correctly.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "zVkXFkjovepb",
    "outputId": "05135012-9d09-4dea-897e-f2bfd1a5c4bd"
   },
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "qKtAZTjdwizO",
    "outputId": "7179d8c0-30c5-4955-9824-65dbb43c2528"
   },
   "outputs": [],
   "source": [
    "df['clean_joined'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "uePzZ830Ur0d",
    "outputId": "3d2dc083-651d-4baf-ee43-fa4d9507bc85"
   },
   "outputs": [],
   "source": [
    "#create a new column in the DataFrame called 'length'\n",
    "df['length'] = df['original'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIOWt7odmdax"
   },
   "source": [
    "## **Data Visualisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72ELduQSuekb"
   },
   "source": [
    "### **Word Count Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "beLFxApfofWS",
    "outputId": "81c94771-498b-408a-940b-07c1fe767082"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame with a 'text' column and a 'label' column\n",
    "df['word_count'] = df['clean_joined'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Plotting word count distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "df[df['LABEL'] == 0]['word_count'].hist(alpha=0.5, color='blue', bins=50, label='1')\n",
    "df[df['LABEL'] == 1]['word_count'].hist(alpha=0.5, color='red', bins=50, label='0')\n",
    "plt.legend()\n",
    "plt.title('Word Count Distribution in Real vs Fake News')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqhAxRFvo_ZZ"
   },
   "source": [
    "### **Explanation:**\n",
    "\n",
    "**Distribution Characteristics:**\n",
    "\n",
    "1. Most articles have a word count less than 1000.\n",
    "2. Real news articles (blue) are generally more frequent across all word count ranges.\n",
    "3. Fake news articles (red) are present but less frequent compared to real news articles.\n",
    "\n",
    "**Insights:**\n",
    "\n",
    "**1. High Concentration of Short Articles:** Both fake and real news have a high concentration of articles with low word counts (0-500 words). This indicates that shorter articles are more common.\n",
    "\n",
    "**2. Longer Articles:** Real news has more instances of longer articles (up to around 4000-5000 words), whereas fake news articles tend to have shorter word counts and fewer long articles.\n",
    "\n",
    "**3. Overlap:** There is significant overlap in the word count distribution of fake and real news, suggesting that word count alone may not be a sufficient distinguishing feature but could still provide useful insights when combined with other features. Therefore, it is essential to use more sophisticated features (like TF-IDF, word embeddings, etc.) in our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kj5EP0q8uluf"
   },
   "source": [
    "### **Word Cloud Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 875
    },
    "id": "MIdOF7n-gO8p",
    "outputId": "1484072f-af8a-4710-e694-5979aab96d94"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Word cloud for fake news\n",
    "fake_news_text = \" \".join(df[df['LABEL'] == 1]['clean_joined'])\n",
    "wordcloud_fake = WordCloud(width=800, height=400, background_color='black').generate(fake_news_text)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud_fake, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Fake News')\n",
    "plt.show()\n",
    "\n",
    "# Word cloud for real news\n",
    "real_news_text = \" \".join(df[df['LABEL'] == 0]['clean_joined'])\n",
    "wordcloud_real = WordCloud(width=800, height=400, background_color='white').generate(real_news_text)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud_real, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Real News')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPU8OOsonEiT"
   },
   "source": [
    "### **Analysis of the Word Cloud for fake news**\n",
    "\n",
    "**Prominent Words:**\n",
    "\n",
    "1. Clinton and Trump are the most prominent words, indicating that these names appear frequently in fake news articles. This suggests that fake news often targets or involves prominent political figures.\n",
    "2. Other significant words include year, people, president, candidate, state, and campaign.\n",
    "\n",
    "**Political Focus:**\n",
    "\n",
    "1. Many of the words are related to politics, such as democrat, republican, government, election, voter, and political. This indicates a heavy focus on political topics in fake news content.\n",
    "\n",
    "**Common Topics:**\n",
    "\n",
    "1. Words like country, vote, support, issue, debate, attack, and campaign suggest that common topics in fake news include political campaigns, elections, and various political issues.\n",
    "\n",
    "**Context and Implications:**\n",
    "\n",
    "1. The prevalence of words such as obama, bush, rubio, and cruz indicates that fake news articles may discuss a range of political figures and not just the most current or prominent ones.\n",
    "2. Words like fact, says, likely, and issue might be used in attempts to lend credibility to the fake news articles, making them appear more legitimate or factual to readers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fXlJLYHnlX8"
   },
   "source": [
    "### **Analysis of the Word Cloud for Real News**\n",
    "\n",
    "**Prominent Words:**\n",
    "\n",
    "1. Similar to the fake news word cloud, year, people, time, trump, and clinton are prominent, indicating these are common topics in real news articles as well.\n",
    "\n",
    "**Political Focus:**\n",
    "\n",
    "1. Words such as government, election, president, russia, american, state, and political indicate that real news articles also focus heavily on political topics.\n",
    "\n",
    "**Common Topics:**\n",
    "\n",
    "1. Terms like russia, world, problem, issue, military, public, and power suggest that real news covers a wide range of global and national issues, including international relations, public affairs, and governance.\n",
    "\n",
    "**Comparison with Fake News:**\n",
    "\n",
    "1. Both word clouds have a significant overlap in terms of the most frequent words, such as clinton, trump, people, year, time, and government. This indicates that both fake and real news articles discuss similar high-profile topics.\n",
    "\n",
    "**The fake news word cloud seems to have a heavier focus on individual political figures and sensational terms, while the real news word cloud includes broader topics like world, problem, issue, and public.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbBrGXFkAWwz"
   },
   "source": [
    "## **Count Vectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM-DwAARK-0X"
   },
   "source": [
    "The primary purpose of Count Vectorizer is to convert a collection of text documents into a matrix of token counts, which can then be used as input for machine learning algorithms.\n",
    "\n",
    "When you use the Count Vectorizer, it tokenizes each document, builds a vocabulary of all the tokens (words) present across all documents, and then counts the occurrences of each token in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DTH2FN4_L7B",
    "outputId": "86309e9d-c901-4473-bfcb-ed779593e11f"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer().fit(df.clean_joined)\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "lNv_67xe_yZm",
    "outputId": "523541fe-dffe-472b-a155-94be7704c20e"
   },
   "outputs": [],
   "source": [
    "example = df['original'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Amoix7rp_f5y",
    "outputId": "a0bc54f5-7c42-4189-fae4-f73e2c482de5"
   },
   "outputs": [],
   "source": [
    "vec = cv.transform([example])\n",
    "print(vec)\n",
    "print(vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeGpRNmFr5W7",
    "outputId": "8758ad1f-8a68-4f8d-a3ee-cd1eaaa20816"
   },
   "outputs": [],
   "source": [
    "# Create a reverse dictionary to map indices to tokens\n",
    "reverse_vocabulary = {index: token for token, index in cv.vocabulary_.items()}\n",
    "\n",
    "# Find the token with index 111\n",
    "token_with_index_111 = reverse_vocabulary[111]\n",
    "print(token_with_index_111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wM9uGwKdSJql",
    "outputId": "df07477b-e211-4f82-e077-d9afd9565cb9"
   },
   "outputs": [],
   "source": [
    "# New variable to hold the transformed versions of the news.\n",
    "news_vec = cv.transform(df.clean_joined)\n",
    "print(news_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNHUkRlHsVSy"
   },
   "source": [
    "Note: Each row corresponds to a document, each column corresponds to a token from the vocabulary, and the values represent the count of the token in the respective document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1CdwYEpSRkJ"
   },
   "source": [
    "## **TFIDF Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoQjKAQAOGwo",
    "outputId": "77a598c6-b97b-4e7e-bb99-b4daee6208a0"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Extract TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf_vectorizer.fit_transform(df['clean_joined'])\n",
    "X[:5].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AiisHBrrOMJG",
    "outputId": "3636dbc7-3a6f-4c10-dcbf-055b3b2f58be"
   },
   "outputs": [],
   "source": [
    "y=df['LABEL']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hvuadrckd_4c"
   },
   "source": [
    "## **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_b0V-tyAvU7I",
    "outputId": "9d1406d1-2ade-4641-c73b-01b5cee70321"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2C_wNedobYx"
   },
   "outputs": [],
   "source": [
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VuWAxgmiu1gC",
    "outputId": "97491221-f67c-4c87-de9c-828317cf3ea3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcktdt7QNFpc",
    "outputId": "b61eeca3-bc8c-45ed-cd13-d01793f3f17a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(classification_report(y_pred,y_test))\n",
    "print(confusion_matrix(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3JOqF0dNUcn",
    "outputId": "dc29d9fd-76aa-4575-ea16-34aef9b31344"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PulxVnqHV9FB"
   },
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crCbUkdeWCY2"
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "models = {\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"XGBoost\": XGBClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aI0VgYTuWjb4"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    results[model_name] = evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjziqdGJWlBQ",
    "outputId": "004c1d67-c9df-4532-f574-22f940642e74"
   },
   "outputs": [],
   "source": [
    "# Print the evaluation results\n",
    "for model_name, (accuracy, precision, recall, f1) in results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"  Precision: {precision:.2f}\")\n",
    "    print(f\"  Recall: {recall:.2f}\")\n",
    "    print(f\"  F1 Score: {f1:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qvo1FKYOOK5A"
   },
   "outputs": [],
   "source": [
    "# Define the XGBoost model\n",
    "XGB_model = XGBClassifier()\n",
    "XGB_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sFR2QqL5OUqc",
    "outputId": "5eead845-aed3-44fd-ff96-446ecab0f40c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGYuku5pOlfv",
    "outputId": "38b12c96-0193-4ff4-a79f-cedb2e063e6c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbA7xeDUVoCZ"
   },
   "source": [
    "## **STREAMLIT APP**\n",
    "\n",
    "Streamlit is an open-source Python framework for data scientists and AI/ML engineers to deliver dynamic data apps with only a few lines of code. Build and deploy powerful data apps in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUCD5NY9Vnnn",
    "outputId": "11b1207a-a43f-4f1e-fb3e-5f778fcbd30c"
   },
   "outputs": [],
   "source": [
    "!pip install streamlit\n",
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39lUyVaDXcaq",
    "outputId": "063f256d-f06d-4617-94d0-5c539c9fe6c6"
   },
   "outputs": [],
   "source": [
    "!pip install streamlit --quiet\n",
    "!npm install -g localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mn7ij7BWNnM",
    "outputId": "00be3bee-c34a-40ac-d3c4-ae4343909e61"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Save the model and vectorizer\n",
    "joblib.dump(XGB_model, 'xgb_news_classifier_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOJ2Y2YXMykb",
    "outputId": "f5b1144a-a91a-4b93-d2bc-061ed1a79a32"
   },
   "outputs": [],
   "source": [
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vs_6EdiOWnF4",
    "outputId": "32143587-a808-4c06-883c-ae42027f2412"
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import gensim\n",
    "\n",
    "# Load the saved XGBoost model and vectorizer\n",
    "model = joblib.load('xgb_news_classifier_model.pkl')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "# Preprocessing function using Gensim\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):  # Tokenization\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Fake News Classifier\")\n",
    "\n",
    "st.write(\"Enter the news title and text to classify it as real or fake.\")\n",
    "\n",
    "# Input text from the user\n",
    "title = st.text_input(\"Title\")\n",
    "text = st.text_area(\"Text\")\n",
    "\n",
    "# Combine the title and text\n",
    "if st.button(\"Classify\"):\n",
    "    if title and text:\n",
    "        original_text = title + ' ' + text\n",
    "        clean_text = preprocess(original_text)\n",
    "        clean_joined = ' '.join(clean_text)\n",
    "\n",
    "        # Transform the input text using the TF-IDF vectorizer\n",
    "        text_vector = vectorizer.transform([clean_joined])\n",
    "\n",
    "        # Predict the label\n",
    "        prediction = model.predict(text_vector)[0]\n",
    "\n",
    "        # Display the result\n",
    "        if prediction == 1:\n",
    "            st.success(\"This news is Real.\")\n",
    "        else:\n",
    "            st.error(\"This news is Fake.\")\n",
    "    else:\n",
    "        st.warning(\"Please enter both title and text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CH5jgSDMXJ2R",
    "outputId": "1193f157-f263-4194-eeaa-19a82f196e99"
   },
   "outputs": [],
   "source": [
    "!wget -q -O - ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ovcnj1oBXkWf",
    "outputId": "61c9c6a8-5120-4935-d56a-39114778d5bd"
   },
   "outputs": [],
   "source": [
    "! streamlit run app.py & npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
